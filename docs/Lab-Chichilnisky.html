

<style>

</style>

<div id="fig_el229181397327283098007929984303"></div>
<script>
function mpld3_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(mpld3) !== "undefined" && mpld3._mpld3IsLoaded){
   // already loaded: just create the figure
   !function(mpld3){
       
       mpld3.draw_figure("fig_el229181397327283098007929984303", {"data": {"data01": [[-0.16453609496786936, 0.35752138090594815], [-0.3139186014518313, -0.04920135287055534], [0.5279575801330681, -0.038569538501021725], [0.6795870104269536, -0.11325869586794159], [-0.3489102083990806, -0.05547077594418459], [0.4898782902927966, -0.14773888258225235], [-0.23901094465548206, -0.3135745185707696], [-0.2593809335423207, -0.25846057874489786], [-0.4657400899716062, -0.4591691679499384], [0.6070922152394621, -0.058957451850304404], [0.33017217477722827, 0.14529676983840661], [-0.3873909295286093, 0.330222645674363], [-0.1228368616147775, -0.029152647585164095], [-0.1403827650761393, 0.37422806241749584], [-0.2317480752017401, 0.6105555761769953], [-0.2097252273783428, -0.4687692650788837], [-0.30099828250348554, -0.18800843204068415], [0.5363637424272083, 0.06070834098736243], [0.18937276517854965, 0.19868703207419464], [-0.28976137017423254, 0.00011657687201728174], [0.5343240973453061, -0.08315886012700695], [-0.13029175156164688, -0.026431084655480382], [-0.04364633751516891, 0.06120578677773183], [-0.24646940227824, 0.15137908064457045]]}, "height": 1080.0, "plugins": [{"type": "reset"}, {"type": "zoom", "enabled": false, "button": true}, {"type": "boxzoom", "enabled": false, "button": true}, {"hoffset": 0, "id": "el22918139732712536440", "location": "mouse", "type": "tooltip", "voffset": 10, "labels": ["the glibc is a core component of the system that cannot be nor loaded as a basically have options recompile elastix from source if you have access to or get a binary that was compiled for Red Hat CentOS what Sherlock try to run it on Sherlock for more Kilian", "Data moves are in a great place now with plenty of extra Thanks for all your help with The only issue left on the stack is users and ej", "Hi going to close out this I still have the hard down boot drive ticket to remind me that waiting on the reply on the other threads as Have a good Karl Kornel System AdministratorResearch Computing Stanford", "Hi has been and is back Karl Kornel System AdministratorResearch Computing Stanford", "people are using iko right now to get work done because we have no space on I agree with letting it sit for ej", "Hi going to close out this in favor of the which has my a clearer make sure that your questions from here are answered Karl Kornel System AdministratorResearch Computing Stanford", "Hi EJ system was up it and the console was kind of I rebooted it and applied the latest FreeNAS updates and rebooted it online", "Thank We will make sure to reduce the frequency of this kind of request moving ej", "Call We will keep it ej", "Have a good Karl Kornel System AdministratorResearch Computing Stanford", "Hi message is generated by a tool called when the system was first up to do system patching on a regular there are certain patches which require reboots in order for the updated software to take When said patches are that warning will so that you will know to schedule a reboot sometime for now the question is It looks like Nishal overloaded so been and so the message should go Karl Kornel System AdministratorResearch Computing Stanford", "Could these SUNetIDs be added to the list of people who can log in to our dantemur staffa ej", "During setup I had the emails just going to me as there were quite a It will now send emails to the", "Its working Thanks a lot for the prompt Nishal", "Please disregard the attachment I added it earlier by accident", "That sounds thank ej On Jul at William Law Right now cosmic is transferring data at about to actually a bit more in after a bit of trial and error is a little compressing the Assuming cosmic has TB total and that will take about the data rate into mississippi this Cosmic has been up for for what that is After the initial sync is the deltas should be extremely For on the difference between the two snapshots that are currently present is quite NAME USED AVAIL REFER MOUNTPOINT once we are over the initial bump it will be quite Rsync will likely be slower tho it depends on In the long run the should be much shorter as just the changes between snapshots will be keep an eye on go ahead and get roses going today Will Click to give William Law On Jul at PM ej OK then be could you ballpark the expected copy and compare to the typical It would be silly to copy for weeks then lose it all on a reboot and start If rather just do rsync and get our data backed up ej On Jul at William Law short answer is not Longer answer You can monitor network traffic on mississippi with systat You can periodically run zfs list to see the dataset increasing in size on OR connect to cosmic and see that the session is running in going but not at as fast as I would try setting up replication on roses with the which I hope will be faster as they run it through If it is interrupted it will have to be redone from the previous Newer versions of zfs the command line utilities on cosmic and support resume from a failed but I could not get it to work between the two machines got out of memory It sounds like it might work in FreeNAS but that is just a few weeks Will Click to give William Law On Jul at AM ej Is there an easy way to monitor the make sure chugging what happens if one of the machines has to reboot during this very long ej On Jul at William Law it is currently a partial dataset so you cd to You will be able to once it Will Click to give William Law On July at AM ej I see a directory unable to cd into it When there will be a full directory there with normal folders and files ej On Jul at William Law the transfer is in progress from cosmic to the command I ended up using zfs send ssh recv Will Click to give William Law On July at AM William Law we absolutely have to remove them leaving them in place is It could free up leave that in place and transfer the existing make a new snapshot with the same naming format and todays Will On July at AM ej Totally OK but do we have to remove our rollback ej On Jul at William Law There is an existing snapshot on cosmic for each Is it OK to remove those and create one for Will Click to give William Law On July at AM William Law I think one way is exactly right or Personally go the zfs send route as it is better in the long It is difficult to get started give it a try if that is I probably had some syntax issues this Will On July at AM ej Is this the right rather than rsync which I use for everything Despite the I did not intend an arrangement that relies on ZFS ej On Jul at William Law Hi EJ The datasets are named and Replica so you need to drop the It should be like zfs create zfs send ssh arcfour recv You could recv it to but that would place the replica directly in hope that makes You can see the dataset names on the system with zfs FreeNAS uses the property is used to tell the system to mount the dataset on To make a you can either do it through the gui or simply zfs snapshot You can list for example of the dataset zfs list snap Note that to do incremental sends in the you need to keep the snapshot on the source and Then you can zfs send ssh arcfour recv Will Click to give William Law On Jul at PM ej Like On zfs create On zfs send ssh arcfour recv ej On Jul at William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will Click to give William Law On July at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On July at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On July at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On July at PM ej ej On July at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On July at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On June at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On June at PM ej for putting us your schedule for the check in ej On June at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On June at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On April at AM ej hi the ej On April at PM ej Thanks for handling this so will stay tuned for updates ej On April at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On April at PM ej Ah OK Same login credentials as other ej On April at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On April at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On April at PM ej Now that our rack is set can we get the new server in ej On April at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On Jul at AM William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will On Jul at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On Jul at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On Jul at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On Jul at PM ej ej On Jul at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On Jul at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On Jun at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On Jun at PM ej for putting us your schedule for the check in ej On Jun at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On Jun at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On Apr at AM ej hi the ej On Apr at PM ej Thanks for handling this so will stay tuned for updates ej On Apr at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On Apr at PM ej Ah OK Same login credentials as other ej On Apr at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On Apr at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On Apr at PM ej Now that our rack is set can we get the new server in ej On Apr at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On July at AM William Law Hi EJ The datasets are named and Replica so you need to drop the It should be like zfs create zfs send ssh arcfour recv You could recv it to but that would place the replica directly in hope that makes You can see the dataset names on the system with zfs FreeNAS uses the property is used to tell the system to mount the dataset on To make a you can either do it through the gui or simply zfs snapshot You can list for example of the dataset zfs list snap Note that to do incremental sends in the you need to keep the snapshot on the source and Then you can zfs send ssh arcfour recv Will On July at PM ej Like On zfs create On zfs send ssh arcfour recv ej On Jul at William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will Click to give William Law On July at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On July at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On July at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On July at PM ej ej On July at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On July at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On June at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On June at PM ej for putting us your schedule for the check in ej On June at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On June at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On April at AM ej hi the ej On April at PM ej Thanks for handling this so will stay tuned for updates ej On April at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On April at PM ej Ah OK Same login credentials as other ej On April at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On April at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On April at PM ej Now that our rack is set can we get the new server in ej On April at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On July at AM William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to", "Sounds I would say do this as soon as you Cosmic is rarely for in the worst some analysis code hangs up and waits for NFS to come ej", "Hi turns out the Toshiba drive is still under so if you like I can hand it off to you for Here are the drive was manufactured May Karl Kornel System AdministratorResearch Computing Stanford", "The copy back USB drive to JBOD will finish up before so change on August to AM on August I did notice that one drive was reporting high That was the USB It was sitting on top of a not in a chassis so It have any like a chassis It hit at least outside SRCF It was working really basically at as data is copied off of drive temperature peaked somewhere around which is one of its failure actually writing this from where opened up an air channel for the The now down to F and dropping back down to OK using the drive for the next few but after I think going to dispose of it and replace it with one of the spare drives you other drives in the chassis are reading around which is Karl Kornel System AdministratorResearch Computing Stanford even though not planning on doing the cosmic reboot right The reason I want to interrupt the transfer off of the USB", "This is our only significant issue moving I think the right plan is a combo of and SUNetID authentication on our compute servers and filers one group of trusted personnel on these machines individual file permissions default to one shared user in this on all servers NFS mounts on filers are Not but seems easy to Let me know if you have a better ej", "Hi to held off for skip Friday and wait to hear from you next Karl Kornel System AdministratorResearch Computing Stanford", "Hi absolutely right that better to use SUNetIDs instead of custom I would also extend that to say that better to also use UIDs user ID instead of since NFS really likes to use those only certain modes of NFS and above that support usernames instead of I have another SoM lab with Linux except they originally decided to spin up their own LDAP with their own usernames and user Changing usernames to SUNetIDs was easy to and was accomplished changing UIDs is You have to locate every file using the old and then change it to the new For that they are stuck with their LDAP using common SUNetIDs and UIDs is the best thing to in my to the storage Although FreeNAS is able to use LDAP for user FreeNAS currently has an issue with Kerberos this issue is preventing us from directly accessing Central LDAP from I have a FreeNAS bug open FreeNAS currently able to talk to Central LDAP one of the servers would have to fill that But that if someone overloads the LDAP would suddenly stop the best thing to do there would be to wait for FreeNAS to come out the bug is fixed in and then to upgrade the filers to the filers are then permissions might need to be but I looked into it that far yet hit the FreeNAS all I have for get back to you with more info Karl Kornel System AdministratorResearch Computing Stanford", "Hi Thank I have passed along the request for I will let you know ASAP if there are any Thanks Jill Jill RLAT Laboratory Manager Neurosurgery School of Medicine Stanford University", "super If something that can be taken care of and an admin is online and one so how do I deal with such a we expect it to be a small config but we know how to do Note that issues almost always require us to engage the campus networking team and not sure what their options are for support for situations like Can you put me in touch with someone there so I can t might very well be possible to contract with another group in IT for that occasional at an hourly I can inquire for That would be Or you may want to have one of your team members go through the process to get access to SLAC and to the SRCF so they can pop over and do a reboot if that is do that if we cover it as Again if you or someone can put me in touch with the right people and give your permission that would be ej"]}], "id": "el22918139732728309800", "width": 1080.0, "axes": [{"ydomain": [-0.6000000000000001, 0.8], "id": "el22918139732723807120", "zoomable": true, "texts": [{"zorder": 3, "text": "Lab- Chichilnisky Ticket Groupings", "id": "el22918139732703424640", "rotation": -0.0, "v_baseline": "auto", "alpha": 1, "h_anchor": "middle", "coordinates": "axes", "fontsize": 20.0, "color": "#000000", "position": [0.49999999999999994, 1.0059737156511348]}], "sharey": [], "yscale": "linear", "axes": [{"scale": "linear", "fontsize": 10.0, "tickformat": null, "tickvalues": null, "position": "bottom", "grid": {"gridOn": false}, "nticks": 10, "visible": true}, {"scale": "linear", "fontsize": 10.0, "tickformat": null, "tickvalues": null, "position": "left", "grid": {"gridOn": false}, "nticks": 10, "visible": true}], "sharex": [], "lines": [], "xdomain": [-0.6000000000000001, 0.8], "markers": [], "collections": [{"edgewidths": [1.0], "zorder": 1, "pathcoordinates": "display", "id": "el22918139732712536440", "facecolors": ["#FFA256", "#FF562B", "#4756FB", "#2ADCDC", "#9CFAA3", "#2ADCDC", "#9CFAA3", "#62FAC3", "#9CFAA3", "#2ADCDC", "#2ADCDC", "#7F00FF", "#9CFAA3", "#FF562B", "#7F00FF", "#9CFAA3", "#9CFAA3", "#2ADCDC", "#4756FB", "#0FA2EF", "#2ADCDC", "#0FA2EF", "#D4DC7F", "#FF0000"], "pathtransforms": [[4.47213595499958, 0.0, 0.0, 4.47213595499958, 0.0, 0.0]], "offsets": "data01", "xindex": 0, "alphas": [null], "edgecolors": ["#000000"], "yindex": 1, "offsetcoordinates": "data", "paths": [[[[0.0, -0.5], [0.13260155, -0.5], [0.25978993539242673, -0.44731684579412084], [0.3535533905932738, -0.3535533905932738], [0.44731684579412084, -0.25978993539242673], [0.5, -0.13260155], [0.5, 0.0], [0.5, 0.13260155], [0.44731684579412084, 0.25978993539242673], [0.3535533905932738, 0.3535533905932738], [0.25978993539242673, 0.44731684579412084], [0.13260155, 0.5], [0.0, 0.5], [-0.13260155, 0.5], [-0.25978993539242673, 0.44731684579412084], [-0.3535533905932738, 0.3535533905932738], [-0.44731684579412084, 0.25978993539242673], [-0.5, 0.13260155], [-0.5, 0.0], [-0.5, -0.13260155], [-0.44731684579412084, -0.25978993539242673], [-0.3535533905932738, -0.3535533905932738], [-0.25978993539242673, -0.44731684579412084], [-0.13260155, -0.5], [0.0, -0.5]], ["M", "C", "C", "C", "C", "C", "C", "C", "C", "Z"]]]}], "axesbg": "#FFFFFF", "ylim": [-0.6000000000000001, 0.8], "images": [], "bbox": [0.125, 0.125, 0.775, 0.775], "xscale": "linear", "xlim": [-0.6000000000000001, 0.8], "paths": [], "axesbgalpha": null}]});
   }(mpld3);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/mpld3
   require.config({paths: {d3: "https://mpld3.github.io/js/d3.v3.min"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      mpld3_load_lib("https://mpld3.github.io/js/mpld3.v0.3.js", function(){
         
         mpld3.draw_figure("fig_el229181397327283098007929984303", {"data": {"data01": [[-0.16453609496786936, 0.35752138090594815], [-0.3139186014518313, -0.04920135287055534], [0.5279575801330681, -0.038569538501021725], [0.6795870104269536, -0.11325869586794159], [-0.3489102083990806, -0.05547077594418459], [0.4898782902927966, -0.14773888258225235], [-0.23901094465548206, -0.3135745185707696], [-0.2593809335423207, -0.25846057874489786], [-0.4657400899716062, -0.4591691679499384], [0.6070922152394621, -0.058957451850304404], [0.33017217477722827, 0.14529676983840661], [-0.3873909295286093, 0.330222645674363], [-0.1228368616147775, -0.029152647585164095], [-0.1403827650761393, 0.37422806241749584], [-0.2317480752017401, 0.6105555761769953], [-0.2097252273783428, -0.4687692650788837], [-0.30099828250348554, -0.18800843204068415], [0.5363637424272083, 0.06070834098736243], [0.18937276517854965, 0.19868703207419464], [-0.28976137017423254, 0.00011657687201728174], [0.5343240973453061, -0.08315886012700695], [-0.13029175156164688, -0.026431084655480382], [-0.04364633751516891, 0.06120578677773183], [-0.24646940227824, 0.15137908064457045]]}, "height": 1080.0, "plugins": [{"type": "reset"}, {"type": "zoom", "enabled": false, "button": true}, {"type": "boxzoom", "enabled": false, "button": true}, {"hoffset": 0, "id": "el22918139732712536440", "location": "mouse", "type": "tooltip", "voffset": 10, "labels": ["the glibc is a core component of the system that cannot be nor loaded as a basically have options recompile elastix from source if you have access to or get a binary that was compiled for Red Hat CentOS what Sherlock try to run it on Sherlock for more Kilian", "Data moves are in a great place now with plenty of extra Thanks for all your help with The only issue left on the stack is users and ej", "Hi going to close out this I still have the hard down boot drive ticket to remind me that waiting on the reply on the other threads as Have a good Karl Kornel System AdministratorResearch Computing Stanford", "Hi has been and is back Karl Kornel System AdministratorResearch Computing Stanford", "people are using iko right now to get work done because we have no space on I agree with letting it sit for ej", "Hi going to close out this in favor of the which has my a clearer make sure that your questions from here are answered Karl Kornel System AdministratorResearch Computing Stanford", "Hi EJ system was up it and the console was kind of I rebooted it and applied the latest FreeNAS updates and rebooted it online", "Thank We will make sure to reduce the frequency of this kind of request moving ej", "Call We will keep it ej", "Have a good Karl Kornel System AdministratorResearch Computing Stanford", "Hi message is generated by a tool called when the system was first up to do system patching on a regular there are certain patches which require reboots in order for the updated software to take When said patches are that warning will so that you will know to schedule a reboot sometime for now the question is It looks like Nishal overloaded so been and so the message should go Karl Kornel System AdministratorResearch Computing Stanford", "Could these SUNetIDs be added to the list of people who can log in to our dantemur staffa ej", "During setup I had the emails just going to me as there were quite a It will now send emails to the", "Its working Thanks a lot for the prompt Nishal", "Please disregard the attachment I added it earlier by accident", "That sounds thank ej On Jul at William Law Right now cosmic is transferring data at about to actually a bit more in after a bit of trial and error is a little compressing the Assuming cosmic has TB total and that will take about the data rate into mississippi this Cosmic has been up for for what that is After the initial sync is the deltas should be extremely For on the difference between the two snapshots that are currently present is quite NAME USED AVAIL REFER MOUNTPOINT once we are over the initial bump it will be quite Rsync will likely be slower tho it depends on In the long run the should be much shorter as just the changes between snapshots will be keep an eye on go ahead and get roses going today Will Click to give William Law On Jul at PM ej OK then be could you ballpark the expected copy and compare to the typical It would be silly to copy for weeks then lose it all on a reboot and start If rather just do rsync and get our data backed up ej On Jul at William Law short answer is not Longer answer You can monitor network traffic on mississippi with systat You can periodically run zfs list to see the dataset increasing in size on OR connect to cosmic and see that the session is running in going but not at as fast as I would try setting up replication on roses with the which I hope will be faster as they run it through If it is interrupted it will have to be redone from the previous Newer versions of zfs the command line utilities on cosmic and support resume from a failed but I could not get it to work between the two machines got out of memory It sounds like it might work in FreeNAS but that is just a few weeks Will Click to give William Law On Jul at AM ej Is there an easy way to monitor the make sure chugging what happens if one of the machines has to reboot during this very long ej On Jul at William Law it is currently a partial dataset so you cd to You will be able to once it Will Click to give William Law On July at AM ej I see a directory unable to cd into it When there will be a full directory there with normal folders and files ej On Jul at William Law the transfer is in progress from cosmic to the command I ended up using zfs send ssh recv Will Click to give William Law On July at AM William Law we absolutely have to remove them leaving them in place is It could free up leave that in place and transfer the existing make a new snapshot with the same naming format and todays Will On July at AM ej Totally OK but do we have to remove our rollback ej On Jul at William Law There is an existing snapshot on cosmic for each Is it OK to remove those and create one for Will Click to give William Law On July at AM William Law I think one way is exactly right or Personally go the zfs send route as it is better in the long It is difficult to get started give it a try if that is I probably had some syntax issues this Will On July at AM ej Is this the right rather than rsync which I use for everything Despite the I did not intend an arrangement that relies on ZFS ej On Jul at William Law Hi EJ The datasets are named and Replica so you need to drop the It should be like zfs create zfs send ssh arcfour recv You could recv it to but that would place the replica directly in hope that makes You can see the dataset names on the system with zfs FreeNAS uses the property is used to tell the system to mount the dataset on To make a you can either do it through the gui or simply zfs snapshot You can list for example of the dataset zfs list snap Note that to do incremental sends in the you need to keep the snapshot on the source and Then you can zfs send ssh arcfour recv Will Click to give William Law On Jul at PM ej Like On zfs create On zfs send ssh arcfour recv ej On Jul at William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will Click to give William Law On July at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On July at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On July at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On July at PM ej ej On July at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On July at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On June at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On June at PM ej for putting us your schedule for the check in ej On June at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On June at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On April at AM ej hi the ej On April at PM ej Thanks for handling this so will stay tuned for updates ej On April at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On April at PM ej Ah OK Same login credentials as other ej On April at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On April at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On April at PM ej Now that our rack is set can we get the new server in ej On April at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On Jul at AM William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will On Jul at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On Jul at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On Jul at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On Jul at PM ej ej On Jul at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On Jul at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On Jun at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On Jun at PM ej for putting us your schedule for the check in ej On Jun at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On Jun at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On Apr at AM ej hi the ej On Apr at PM ej Thanks for handling this so will stay tuned for updates ej On Apr at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On Apr at PM ej Ah OK Same login credentials as other ej On Apr at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On Apr at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On Apr at PM ej Now that our rack is set can we get the new server in ej On Apr at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On July at AM William Law Hi EJ The datasets are named and Replica so you need to drop the It should be like zfs create zfs send ssh arcfour recv You could recv it to but that would place the replica directly in hope that makes You can see the dataset names on the system with zfs FreeNAS uses the property is used to tell the system to mount the dataset on To make a you can either do it through the gui or simply zfs snapshot You can list for example of the dataset zfs list snap Note that to do incremental sends in the you need to keep the snapshot on the source and Then you can zfs send ssh arcfour recv Will On July at PM ej Like On zfs create On zfs send ssh arcfour recv ej On Jul at William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will Click to give William Law On July at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On July at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On July at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On July at PM ej ej On July at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On July at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On June at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On June at PM ej for putting us your schedule for the check in ej On June at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On June at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On April at AM ej hi the ej On April at PM ej Thanks for handling this so will stay tuned for updates ej On April at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On April at PM ej Ah OK Same login credentials as other ej On April at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On April at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On April at PM ej Now that our rack is set can we get the new server in ej On April at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On July at AM William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to", "Sounds I would say do this as soon as you Cosmic is rarely for in the worst some analysis code hangs up and waits for NFS to come ej", "Hi turns out the Toshiba drive is still under so if you like I can hand it off to you for Here are the drive was manufactured May Karl Kornel System AdministratorResearch Computing Stanford", "The copy back USB drive to JBOD will finish up before so change on August to AM on August I did notice that one drive was reporting high That was the USB It was sitting on top of a not in a chassis so It have any like a chassis It hit at least outside SRCF It was working really basically at as data is copied off of drive temperature peaked somewhere around which is one of its failure actually writing this from where opened up an air channel for the The now down to F and dropping back down to OK using the drive for the next few but after I think going to dispose of it and replace it with one of the spare drives you other drives in the chassis are reading around which is Karl Kornel System AdministratorResearch Computing Stanford even though not planning on doing the cosmic reboot right The reason I want to interrupt the transfer off of the USB", "This is our only significant issue moving I think the right plan is a combo of and SUNetID authentication on our compute servers and filers one group of trusted personnel on these machines individual file permissions default to one shared user in this on all servers NFS mounts on filers are Not but seems easy to Let me know if you have a better ej", "Hi to held off for skip Friday and wait to hear from you next Karl Kornel System AdministratorResearch Computing Stanford", "Hi absolutely right that better to use SUNetIDs instead of custom I would also extend that to say that better to also use UIDs user ID instead of since NFS really likes to use those only certain modes of NFS and above that support usernames instead of I have another SoM lab with Linux except they originally decided to spin up their own LDAP with their own usernames and user Changing usernames to SUNetIDs was easy to and was accomplished changing UIDs is You have to locate every file using the old and then change it to the new For that they are stuck with their LDAP using common SUNetIDs and UIDs is the best thing to in my to the storage Although FreeNAS is able to use LDAP for user FreeNAS currently has an issue with Kerberos this issue is preventing us from directly accessing Central LDAP from I have a FreeNAS bug open FreeNAS currently able to talk to Central LDAP one of the servers would have to fill that But that if someone overloads the LDAP would suddenly stop the best thing to do there would be to wait for FreeNAS to come out the bug is fixed in and then to upgrade the filers to the filers are then permissions might need to be but I looked into it that far yet hit the FreeNAS all I have for get back to you with more info Karl Kornel System AdministratorResearch Computing Stanford", "Hi Thank I have passed along the request for I will let you know ASAP if there are any Thanks Jill Jill RLAT Laboratory Manager Neurosurgery School of Medicine Stanford University", "super If something that can be taken care of and an admin is online and one so how do I deal with such a we expect it to be a small config but we know how to do Note that issues almost always require us to engage the campus networking team and not sure what their options are for support for situations like Can you put me in touch with someone there so I can t might very well be possible to contract with another group in IT for that occasional at an hourly I can inquire for That would be Or you may want to have one of your team members go through the process to get access to SLAC and to the SRCF so they can pop over and do a reboot if that is do that if we cover it as Again if you or someone can put me in touch with the right people and give your permission that would be ej"]}], "id": "el22918139732728309800", "width": 1080.0, "axes": [{"ydomain": [-0.6000000000000001, 0.8], "id": "el22918139732723807120", "zoomable": true, "texts": [{"zorder": 3, "text": "Lab- Chichilnisky Ticket Groupings", "id": "el22918139732703424640", "rotation": -0.0, "v_baseline": "auto", "alpha": 1, "h_anchor": "middle", "coordinates": "axes", "fontsize": 20.0, "color": "#000000", "position": [0.49999999999999994, 1.0059737156511348]}], "sharey": [], "yscale": "linear", "axes": [{"scale": "linear", "fontsize": 10.0, "tickformat": null, "tickvalues": null, "position": "bottom", "grid": {"gridOn": false}, "nticks": 10, "visible": true}, {"scale": "linear", "fontsize": 10.0, "tickformat": null, "tickvalues": null, "position": "left", "grid": {"gridOn": false}, "nticks": 10, "visible": true}], "sharex": [], "lines": [], "xdomain": [-0.6000000000000001, 0.8], "markers": [], "collections": [{"edgewidths": [1.0], "zorder": 1, "pathcoordinates": "display", "id": "el22918139732712536440", "facecolors": ["#FFA256", "#FF562B", "#4756FB", "#2ADCDC", "#9CFAA3", "#2ADCDC", "#9CFAA3", "#62FAC3", "#9CFAA3", "#2ADCDC", "#2ADCDC", "#7F00FF", "#9CFAA3", "#FF562B", "#7F00FF", "#9CFAA3", "#9CFAA3", "#2ADCDC", "#4756FB", "#0FA2EF", "#2ADCDC", "#0FA2EF", "#D4DC7F", "#FF0000"], "pathtransforms": [[4.47213595499958, 0.0, 0.0, 4.47213595499958, 0.0, 0.0]], "offsets": "data01", "xindex": 0, "alphas": [null], "edgecolors": ["#000000"], "yindex": 1, "offsetcoordinates": "data", "paths": [[[[0.0, -0.5], [0.13260155, -0.5], [0.25978993539242673, -0.44731684579412084], [0.3535533905932738, -0.3535533905932738], [0.44731684579412084, -0.25978993539242673], [0.5, -0.13260155], [0.5, 0.0], [0.5, 0.13260155], [0.44731684579412084, 0.25978993539242673], [0.3535533905932738, 0.3535533905932738], [0.25978993539242673, 0.44731684579412084], [0.13260155, 0.5], [0.0, 0.5], [-0.13260155, 0.5], [-0.25978993539242673, 0.44731684579412084], [-0.3535533905932738, 0.3535533905932738], [-0.44731684579412084, 0.25978993539242673], [-0.5, 0.13260155], [-0.5, 0.0], [-0.5, -0.13260155], [-0.44731684579412084, -0.25978993539242673], [-0.3535533905932738, -0.3535533905932738], [-0.25978993539242673, -0.44731684579412084], [-0.13260155, -0.5], [0.0, -0.5]], ["M", "C", "C", "C", "C", "C", "C", "C", "C", "Z"]]]}], "axesbg": "#FFFFFF", "ylim": [-0.6000000000000001, 0.8], "images": [], "bbox": [0.125, 0.125, 0.775, 0.775], "xscale": "linear", "xlim": [-0.6000000000000001, 0.8], "paths": [], "axesbgalpha": null}]});
      });
    });
}else{
    // require.js not available: dynamically load d3 & mpld3
    mpld3_load_lib("https://mpld3.github.io/js/d3.v3.min.js", function(){
         mpld3_load_lib("https://mpld3.github.io/js/mpld3.v0.3.js", function(){
                 
                 mpld3.draw_figure("fig_el229181397327283098007929984303", {"data": {"data01": [[-0.16453609496786936, 0.35752138090594815], [-0.3139186014518313, -0.04920135287055534], [0.5279575801330681, -0.038569538501021725], [0.6795870104269536, -0.11325869586794159], [-0.3489102083990806, -0.05547077594418459], [0.4898782902927966, -0.14773888258225235], [-0.23901094465548206, -0.3135745185707696], [-0.2593809335423207, -0.25846057874489786], [-0.4657400899716062, -0.4591691679499384], [0.6070922152394621, -0.058957451850304404], [0.33017217477722827, 0.14529676983840661], [-0.3873909295286093, 0.330222645674363], [-0.1228368616147775, -0.029152647585164095], [-0.1403827650761393, 0.37422806241749584], [-0.2317480752017401, 0.6105555761769953], [-0.2097252273783428, -0.4687692650788837], [-0.30099828250348554, -0.18800843204068415], [0.5363637424272083, 0.06070834098736243], [0.18937276517854965, 0.19868703207419464], [-0.28976137017423254, 0.00011657687201728174], [0.5343240973453061, -0.08315886012700695], [-0.13029175156164688, -0.026431084655480382], [-0.04364633751516891, 0.06120578677773183], [-0.24646940227824, 0.15137908064457045]]}, "height": 1080.0, "plugins": [{"type": "reset"}, {"type": "zoom", "enabled": false, "button": true}, {"type": "boxzoom", "enabled": false, "button": true}, {"hoffset": 0, "id": "el22918139732712536440", "location": "mouse", "type": "tooltip", "voffset": 10, "labels": ["the glibc is a core component of the system that cannot be nor loaded as a basically have options recompile elastix from source if you have access to or get a binary that was compiled for Red Hat CentOS what Sherlock try to run it on Sherlock for more Kilian", "Data moves are in a great place now with plenty of extra Thanks for all your help with The only issue left on the stack is users and ej", "Hi going to close out this I still have the hard down boot drive ticket to remind me that waiting on the reply on the other threads as Have a good Karl Kornel System AdministratorResearch Computing Stanford", "Hi has been and is back Karl Kornel System AdministratorResearch Computing Stanford", "people are using iko right now to get work done because we have no space on I agree with letting it sit for ej", "Hi going to close out this in favor of the which has my a clearer make sure that your questions from here are answered Karl Kornel System AdministratorResearch Computing Stanford", "Hi EJ system was up it and the console was kind of I rebooted it and applied the latest FreeNAS updates and rebooted it online", "Thank We will make sure to reduce the frequency of this kind of request moving ej", "Call We will keep it ej", "Have a good Karl Kornel System AdministratorResearch Computing Stanford", "Hi message is generated by a tool called when the system was first up to do system patching on a regular there are certain patches which require reboots in order for the updated software to take When said patches are that warning will so that you will know to schedule a reboot sometime for now the question is It looks like Nishal overloaded so been and so the message should go Karl Kornel System AdministratorResearch Computing Stanford", "Could these SUNetIDs be added to the list of people who can log in to our dantemur staffa ej", "During setup I had the emails just going to me as there were quite a It will now send emails to the", "Its working Thanks a lot for the prompt Nishal", "Please disregard the attachment I added it earlier by accident", "That sounds thank ej On Jul at William Law Right now cosmic is transferring data at about to actually a bit more in after a bit of trial and error is a little compressing the Assuming cosmic has TB total and that will take about the data rate into mississippi this Cosmic has been up for for what that is After the initial sync is the deltas should be extremely For on the difference between the two snapshots that are currently present is quite NAME USED AVAIL REFER MOUNTPOINT once we are over the initial bump it will be quite Rsync will likely be slower tho it depends on In the long run the should be much shorter as just the changes between snapshots will be keep an eye on go ahead and get roses going today Will Click to give William Law On Jul at PM ej OK then be could you ballpark the expected copy and compare to the typical It would be silly to copy for weeks then lose it all on a reboot and start If rather just do rsync and get our data backed up ej On Jul at William Law short answer is not Longer answer You can monitor network traffic on mississippi with systat You can periodically run zfs list to see the dataset increasing in size on OR connect to cosmic and see that the session is running in going but not at as fast as I would try setting up replication on roses with the which I hope will be faster as they run it through If it is interrupted it will have to be redone from the previous Newer versions of zfs the command line utilities on cosmic and support resume from a failed but I could not get it to work between the two machines got out of memory It sounds like it might work in FreeNAS but that is just a few weeks Will Click to give William Law On Jul at AM ej Is there an easy way to monitor the make sure chugging what happens if one of the machines has to reboot during this very long ej On Jul at William Law it is currently a partial dataset so you cd to You will be able to once it Will Click to give William Law On July at AM ej I see a directory unable to cd into it When there will be a full directory there with normal folders and files ej On Jul at William Law the transfer is in progress from cosmic to the command I ended up using zfs send ssh recv Will Click to give William Law On July at AM William Law we absolutely have to remove them leaving them in place is It could free up leave that in place and transfer the existing make a new snapshot with the same naming format and todays Will On July at AM ej Totally OK but do we have to remove our rollback ej On Jul at William Law There is an existing snapshot on cosmic for each Is it OK to remove those and create one for Will Click to give William Law On July at AM William Law I think one way is exactly right or Personally go the zfs send route as it is better in the long It is difficult to get started give it a try if that is I probably had some syntax issues this Will On July at AM ej Is this the right rather than rsync which I use for everything Despite the I did not intend an arrangement that relies on ZFS ej On Jul at William Law Hi EJ The datasets are named and Replica so you need to drop the It should be like zfs create zfs send ssh arcfour recv You could recv it to but that would place the replica directly in hope that makes You can see the dataset names on the system with zfs FreeNAS uses the property is used to tell the system to mount the dataset on To make a you can either do it through the gui or simply zfs snapshot You can list for example of the dataset zfs list snap Note that to do incremental sends in the you need to keep the snapshot on the source and Then you can zfs send ssh arcfour recv Will Click to give William Law On Jul at PM ej Like On zfs create On zfs send ssh arcfour recv ej On Jul at William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will Click to give William Law On July at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On July at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On July at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On July at PM ej ej On July at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On July at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On June at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On June at PM ej for putting us your schedule for the check in ej On June at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On June at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On April at AM ej hi the ej On April at PM ej Thanks for handling this so will stay tuned for updates ej On April at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On April at PM ej Ah OK Same login credentials as other ej On April at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On April at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On April at PM ej Now that our rack is set can we get the new server in ej On April at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On Jul at AM William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will On Jul at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On Jul at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On Jul at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On Jul at PM ej ej On Jul at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On Jul at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On Jun at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On Jun at PM ej for putting us your schedule for the check in ej On Jun at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On Jun at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On Apr at AM ej hi the ej On Apr at PM ej Thanks for handling this so will stay tuned for updates ej On Apr at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On Apr at PM ej Ah OK Same login credentials as other ej On Apr at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On Apr at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On Apr at PM ej Now that our rack is set can we get the new server in ej On Apr at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On July at AM William Law Hi EJ The datasets are named and Replica so you need to drop the It should be like zfs create zfs send ssh arcfour recv You could recv it to but that would place the replica directly in hope that makes You can see the dataset names on the system with zfs FreeNAS uses the property is used to tell the system to mount the dataset on To make a you can either do it through the gui or simply zfs snapshot You can list for example of the dataset zfs list snap Note that to do incremental sends in the you need to keep the snapshot on the source and Then you can zfs send ssh arcfour recv Will On July at PM ej Like On zfs create On zfs send ssh arcfour recv ej On Jul at William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to Replication tasks can also be configured in the web One thing to be aware of ZFS take longer than rsync because it replays the filesystem the incremental send capability is really fantastic combined with copy on Hope this Will Click to give William Law On July at AM ej hi Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS Can you send an example invocation of Never done For I might replicate on cosmic to on I might merge the contents of and on cosmic and roses into on suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted Will I suppose rsync will allow snapshot If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of OK good ej On July at AM William Law Hi EJ Everything is hooked up to the same If you want full replicas of a given suggest just doing a ZFS suggest sending the files directly from cosmic to mississippi vs copying on bertha or just to eliminate some This would allow us to move snapshots from one system to the other if you wanted If you need to do some then rsync is the way to For the initial sync it matter as no files are in but subsequently suggest adding to better work with the features of Will On July at AM ej Is the link between Mississippi and our other file servers as fast as it could going to be replicating hundreds of TB so it does matter a ej On July at PM ej ej On July at AM William Law Hi I renamed the pool to and mounted it and iko via the fstab on both smokestack and Will On July at AM ej This is thank Could we call this partition Replica mount it so that it appears on bertha and smokestack as a link in is the way we handle add this to fstab to automatically mount at boot also please remount which seems to be not in the fstab but probably should be Once done we can start replicating ej On June at AM William Law Hi EJ The new storage Mississippi is now Right now the data directory is just If fine I can go ahead and mount it on smokestack and but I thought you might want to name it something Will On June at PM ej for putting us your schedule for the check in ej On June at PM William Law Hi EJ I unfortunately have been the bottleneck here and have been in the midst of some issues on other Karl is waiting for me to setup the ZFS I had hoped to get that done today but that look like it will unfortunately traveling for work next week but will work with Karl on it first thing on the I think we can get it all up early that apologies for the endeavor taking so Will On June at AM ej hi Where do we The server has been there for and love to get it up and Please check with Ruth if there is a question about ej On May at AM ej Thanks for the sounds ej On May at AM Karl System Administrator Hi good morning I was checking around this the all cabled to plus one of the ports is connected to the same switch as all the other has been and an IP address Change request has been submitted to Host network be covering that this afternoon or Will will be doing that once network is I believe all the I or Will will update this ticket as things Karl Kornel System Administrator Research Computing Stanford University On May at AM ej Can we get this server up and ej Begin forwarded On April at AM ej hi the ej On April at PM ej Thanks for handling this so will stay tuned for updates ej On April at PM Karl System Administrator Hi a status update for covering the various tasks that need to be and their current DONE Under DONE Under FreeNAS and To be done on the above As each of the items is see updates appear in this Karl Kornel System Administrator Research Computing Stanford University On April at PM ej Ah OK Same login credentials as other ej On April at PM Karl System Administrator Hi The already the server in the attached from top to the servers two and the new storage a switch at the behind the It is but is not yet going to be The power cable needs to cover the depth of the and the network cable needs to cover the height and depth of the because all the drives are accessed through the so the server has to be able to slide out while powered Friday be searching for a network cable that will cover the and putting that do you have a name for it You can look for unused names Karl Kornel System Administrator Research Computing Stanford University On April at PM Phil Reese its all the lowest possible U in the rack now has the large The other guys will have to work with you on what you need for the setup and then bring that system But it is racked powered and ready for that next step Phil On April at PM ej Now that our rack is set can we get the new server in ej On April at PM Karl System Administrator Hi the email thread for the new storage Karl Kornel System Administrator Research Computing Stanford University Powered by On July at AM William Law You need a snapshot the filesystem is consistent to and then you just do The command is something like A simple version of the command would zfs send ssh arcfour recv A fancier version zfs send ssh arcfour recv recursively sends any descendent datasets and Once the replication has completed you can use or sends incremental changes between two upper case version sends all the snapshots between two to send only what has The great part about the thing is the ability to do incremental overwrites the remote so use it with a great deal of The arcfour stuff is a lower quality but much faster within the same switch it should be just disables a What I would do is create a dataset for each of the current systems like the command is zfs create or can be done through the web interface or we can do and then send the file systems to their datasets so on cosmic would be replicated to", "Sounds I would say do this as soon as you Cosmic is rarely for in the worst some analysis code hangs up and waits for NFS to come ej", "Hi turns out the Toshiba drive is still under so if you like I can hand it off to you for Here are the drive was manufactured May Karl Kornel System AdministratorResearch Computing Stanford", "The copy back USB drive to JBOD will finish up before so change on August to AM on August I did notice that one drive was reporting high That was the USB It was sitting on top of a not in a chassis so It have any like a chassis It hit at least outside SRCF It was working really basically at as data is copied off of drive temperature peaked somewhere around which is one of its failure actually writing this from where opened up an air channel for the The now down to F and dropping back down to OK using the drive for the next few but after I think going to dispose of it and replace it with one of the spare drives you other drives in the chassis are reading around which is Karl Kornel System AdministratorResearch Computing Stanford even though not planning on doing the cosmic reboot right The reason I want to interrupt the transfer off of the USB", "This is our only significant issue moving I think the right plan is a combo of and SUNetID authentication on our compute servers and filers one group of trusted personnel on these machines individual file permissions default to one shared user in this on all servers NFS mounts on filers are Not but seems easy to Let me know if you have a better ej", "Hi to held off for skip Friday and wait to hear from you next Karl Kornel System AdministratorResearch Computing Stanford", "Hi absolutely right that better to use SUNetIDs instead of custom I would also extend that to say that better to also use UIDs user ID instead of since NFS really likes to use those only certain modes of NFS and above that support usernames instead of I have another SoM lab with Linux except they originally decided to spin up their own LDAP with their own usernames and user Changing usernames to SUNetIDs was easy to and was accomplished changing UIDs is You have to locate every file using the old and then change it to the new For that they are stuck with their LDAP using common SUNetIDs and UIDs is the best thing to in my to the storage Although FreeNAS is able to use LDAP for user FreeNAS currently has an issue with Kerberos this issue is preventing us from directly accessing Central LDAP from I have a FreeNAS bug open FreeNAS currently able to talk to Central LDAP one of the servers would have to fill that But that if someone overloads the LDAP would suddenly stop the best thing to do there would be to wait for FreeNAS to come out the bug is fixed in and then to upgrade the filers to the filers are then permissions might need to be but I looked into it that far yet hit the FreeNAS all I have for get back to you with more info Karl Kornel System AdministratorResearch Computing Stanford", "Hi Thank I have passed along the request for I will let you know ASAP if there are any Thanks Jill Jill RLAT Laboratory Manager Neurosurgery School of Medicine Stanford University", "super If something that can be taken care of and an admin is online and one so how do I deal with such a we expect it to be a small config but we know how to do Note that issues almost always require us to engage the campus networking team and not sure what their options are for support for situations like Can you put me in touch with someone there so I can t might very well be possible to contract with another group in IT for that occasional at an hourly I can inquire for That would be Or you may want to have one of your team members go through the process to get access to SLAC and to the SRCF so they can pop over and do a reboot if that is do that if we cover it as Again if you or someone can put me in touch with the right people and give your permission that would be ej"]}], "id": "el22918139732728309800", "width": 1080.0, "axes": [{"ydomain": [-0.6000000000000001, 0.8], "id": "el22918139732723807120", "zoomable": true, "texts": [{"zorder": 3, "text": "Lab- Chichilnisky Ticket Groupings", "id": "el22918139732703424640", "rotation": -0.0, "v_baseline": "auto", "alpha": 1, "h_anchor": "middle", "coordinates": "axes", "fontsize": 20.0, "color": "#000000", "position": [0.49999999999999994, 1.0059737156511348]}], "sharey": [], "yscale": "linear", "axes": [{"scale": "linear", "fontsize": 10.0, "tickformat": null, "tickvalues": null, "position": "bottom", "grid": {"gridOn": false}, "nticks": 10, "visible": true}, {"scale": "linear", "fontsize": 10.0, "tickformat": null, "tickvalues": null, "position": "left", "grid": {"gridOn": false}, "nticks": 10, "visible": true}], "sharex": [], "lines": [], "xdomain": [-0.6000000000000001, 0.8], "markers": [], "collections": [{"edgewidths": [1.0], "zorder": 1, "pathcoordinates": "display", "id": "el22918139732712536440", "facecolors": ["#FFA256", "#FF562B", "#4756FB", "#2ADCDC", "#9CFAA3", "#2ADCDC", "#9CFAA3", "#62FAC3", "#9CFAA3", "#2ADCDC", "#2ADCDC", "#7F00FF", "#9CFAA3", "#FF562B", "#7F00FF", "#9CFAA3", "#9CFAA3", "#2ADCDC", "#4756FB", "#0FA2EF", "#2ADCDC", "#0FA2EF", "#D4DC7F", "#FF0000"], "pathtransforms": [[4.47213595499958, 0.0, 0.0, 4.47213595499958, 0.0, 0.0]], "offsets": "data01", "xindex": 0, "alphas": [null], "edgecolors": ["#000000"], "yindex": 1, "offsetcoordinates": "data", "paths": [[[[0.0, -0.5], [0.13260155, -0.5], [0.25978993539242673, -0.44731684579412084], [0.3535533905932738, -0.3535533905932738], [0.44731684579412084, -0.25978993539242673], [0.5, -0.13260155], [0.5, 0.0], [0.5, 0.13260155], [0.44731684579412084, 0.25978993539242673], [0.3535533905932738, 0.3535533905932738], [0.25978993539242673, 0.44731684579412084], [0.13260155, 0.5], [0.0, 0.5], [-0.13260155, 0.5], [-0.25978993539242673, 0.44731684579412084], [-0.3535533905932738, 0.3535533905932738], [-0.44731684579412084, 0.25978993539242673], [-0.5, 0.13260155], [-0.5, 0.0], [-0.5, -0.13260155], [-0.44731684579412084, -0.25978993539242673], [-0.3535533905932738, -0.3535533905932738], [-0.25978993539242673, -0.44731684579412084], [-0.13260155, -0.5], [0.0, -0.5]], ["M", "C", "C", "C", "C", "C", "C", "C", "C", "Z"]]]}], "axesbg": "#FFFFFF", "ylim": [-0.6000000000000001, 0.8], "images": [], "bbox": [0.125, 0.125, 0.775, 0.775], "xscale": "linear", "xlim": [-0.6000000000000001, 0.8], "paths": [], "axesbgalpha": null}]});
            })
         });
}
</script>